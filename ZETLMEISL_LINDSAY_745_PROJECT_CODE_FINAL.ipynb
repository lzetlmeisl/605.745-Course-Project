{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 605.745 Reasoning Under Uncertainty\n",
    "## Final Draft of Project Code\n",
    "## Lindsay Zetlmeisl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries and download NLTK packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Joseph\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Joseph\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from pomegranate import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data from files and combine into one DataFrame\n",
    "\n",
    "For this project, I used one dataset for fake news and another for real news. The fake news dataset that I used is focused on the time around the 2016 election, and while this is a few years old it's the best fake news dataset that could be found. The dataset is called \"Getting Real About Fake News\" and was found on Kaggle (https://www.kaggle.com/mrisdal/fake-news). The code below reads this data from a CSV into a DataFrame. Some of the articles in this dataset are not in English, and since this project focuses on language processing I filtered out everything that was not English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>author</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-10-26T21:41:00.000+03:00</td>\n",
       "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
       "      <td>Print They should pay all the back all the mon...</td>\n",
       "      <td>Barracuda Brigade</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-10-29T08:47:11.259+03:00</td>\n",
       "      <td>Re: Why Did Attorney General Loretta Lynch Ple...</td>\n",
       "      <td>Why Did Attorney General Loretta Lynch Plead T...</td>\n",
       "      <td>reasoning with facts</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-10-31T01:41:49.479+02:00</td>\n",
       "      <td>BREAKING: Weiner Cooperating With FBI On Hilla...</td>\n",
       "      <td>Red State : \\nFox News Sunday reported this mo...</td>\n",
       "      <td>Barracuda Brigade</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-11-01T05:22:00.000+02:00</td>\n",
       "      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n",
       "      <td>Email Kayla Mueller was a prisoner and torture...</td>\n",
       "      <td>Fed Up</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-11-01T21:56:00.000+02:00</td>\n",
       "      <td>FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...</td>\n",
       "      <td>Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...</td>\n",
       "      <td>Fed Up</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12908</th>\n",
       "      <td>2016-10-26T21:28:00.000+03:00</td>\n",
       "      <td>Tesla Earnings Smash Expectations After Dramat...</td>\n",
       "      <td>Oct 26, 2016 4:26 PM 0 SHARES \\nThere was a su...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12909</th>\n",
       "      <td>2016-10-26T23:53:43.161+03:00</td>\n",
       "      <td>Rules For Rulers (Or How The World Really Works)</td>\n",
       "      <td>The following video is a must watch, particula...</td>\n",
       "      <td>Tyler Durden</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12910</th>\n",
       "      <td>2016-10-26T23:53:49.879+03:00</td>\n",
       "      <td>Fact Check: Trump Is Right that Clinton Might ...</td>\n",
       "      <td>She explains : \\nHillary Clinton wants to star...</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12911</th>\n",
       "      <td>2016-10-27T00:20:00.111+03:00</td>\n",
       "      <td>Caught On Tape: ISIS Destroys Iraqi Abrams Wit...</td>\n",
       "      <td>YHC-FTSE Oct 26, 2016 5:14 PM \\nWould have bee...</td>\n",
       "      <td>Tyler Durden</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12912</th>\n",
       "      <td>2016-10-27T00:35:58.174+03:00</td>\n",
       "      <td>ObamaCare Architect Admits \"The Law Is Working...</td>\n",
       "      <td>Oct 26, 2016 5:20 PM 0 \\nSubmitted by Joseph J...</td>\n",
       "      <td>Tyler Durden</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11732 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                date  \\\n",
       "0      2016-10-26T21:41:00.000+03:00   \n",
       "1      2016-10-29T08:47:11.259+03:00   \n",
       "2      2016-10-31T01:41:49.479+02:00   \n",
       "3      2016-11-01T05:22:00.000+02:00   \n",
       "4      2016-11-01T21:56:00.000+02:00   \n",
       "...                              ...   \n",
       "12908  2016-10-26T21:28:00.000+03:00   \n",
       "12909  2016-10-26T23:53:43.161+03:00   \n",
       "12910  2016-10-26T23:53:49.879+03:00   \n",
       "12911  2016-10-27T00:20:00.111+03:00   \n",
       "12912  2016-10-27T00:35:58.174+03:00   \n",
       "\n",
       "                                                   title  \\\n",
       "0      Muslims BUSTED: They Stole Millions In Gov’t B...   \n",
       "1      Re: Why Did Attorney General Loretta Lynch Ple...   \n",
       "2      BREAKING: Weiner Cooperating With FBI On Hilla...   \n",
       "3      PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...   \n",
       "4      FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...   \n",
       "...                                                  ...   \n",
       "12908  Tesla Earnings Smash Expectations After Dramat...   \n",
       "12909   Rules For Rulers (Or How The World Really Works)   \n",
       "12910  Fact Check: Trump Is Right that Clinton Might ...   \n",
       "12911  Caught On Tape: ISIS Destroys Iraqi Abrams Wit...   \n",
       "12912  ObamaCare Architect Admits \"The Law Is Working...   \n",
       "\n",
       "                                                 content  \\\n",
       "0      Print They should pay all the back all the mon...   \n",
       "1      Why Did Attorney General Loretta Lynch Plead T...   \n",
       "2      Red State : \\nFox News Sunday reported this mo...   \n",
       "3      Email Kayla Mueller was a prisoner and torture...   \n",
       "4      Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...   \n",
       "...                                                  ...   \n",
       "12908  Oct 26, 2016 4:26 PM 0 SHARES \\nThere was a su...   \n",
       "12909  The following video is a must watch, particula...   \n",
       "12910  She explains : \\nHillary Clinton wants to star...   \n",
       "12911  YHC-FTSE Oct 26, 2016 5:14 PM \\nWould have bee...   \n",
       "12912  Oct 26, 2016 5:20 PM 0 \\nSubmitted by Joseph J...   \n",
       "\n",
       "                     author label  \n",
       "0         Barracuda Brigade  fake  \n",
       "1      reasoning with facts  fake  \n",
       "2         Barracuda Brigade  fake  \n",
       "3                    Fed Up  fake  \n",
       "4                    Fed Up  fake  \n",
       "...                     ...   ...  \n",
       "12908                   NaN  fake  \n",
       "12909          Tyler Durden  fake  \n",
       "12910     George Washington  fake  \n",
       "12911          Tyler Durden  fake  \n",
       "12912          Tyler Durden  fake  \n",
       "\n",
       "[11732 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake = pd.read_csv(\"fake.csv\")\n",
    "fake = fake[fake[\"language\"] == \"english\"]\n",
    "fake = fake[fake[\"ord_in_thread\"] == 0]\n",
    "fake.rename(columns={'published': 'date', 'text': 'content'}, inplace=True)\n",
    "fake = fake[[\"date\", \"title\", \"content\", \"author\"]]\n",
    "fake[\"label\"] = \"fake\"\n",
    "fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second dataset that I used is the \"All the News\" dataset (https://www.kaggle.com/snapcrack/all-the-news), which is the source of the real news for this project. It contains many different data sources, but six sources that are known to be credible were selected from the data (The Atlantic, CNN, New York Times, NPR, Reuters, and the Washington Post). The date range of this dataset is also much larger than the fake news dataset, so it was filtered down to a smaller date range around the 2016 election time frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = pd.read_csv(\"articles1.csv\")\n",
    "true = pd.concat([true, pd.read_csv(\"articles2.csv\"), pd.read_csv(\"articles3.csv\")], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>author</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>2016-12-21</td>\n",
       "      <td>U.S. Plans to Step Up Military Campaign Agains...</td>\n",
       "      <td>ABU DHABI, United Arab Emirates  —   The Obama...</td>\n",
       "      <td>Michael S. Schmidt and Eric Schmitt</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2551</th>\n",
       "      <td>2016-12-15</td>\n",
       "      <td>272 Slaves Were Sold to Save Georgetown. What ...</td>\n",
       "      <td>WASHINGTON  —   The human cargo was loaded on ...</td>\n",
       "      <td>Rachel L. Swarns</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>2016-10-14</td>\n",
       "      <td>Among Travelers and Commuters, the Homeless St...</td>\n",
       "      <td>Wilson Silva said he knew the homeless situati...</td>\n",
       "      <td>Corey Kilgannon</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>Bus Bombing in Jerusalem Wounds 21 - The New Y...</td>\n",
       "      <td>JERUSALEM  —   A bomb exploded on a bus in Jer...</td>\n",
       "      <td>Isabel Kershner</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2581</th>\n",
       "      <td>2016-10-20</td>\n",
       "      <td>Syria Cease-Fire Crumbles as Bombings Kill Doz...</td>\n",
       "      <td>BEIRUT, Lebanon  —   For 38 straight days, the...</td>\n",
       "      <td>Anne Barnard</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142469</th>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>This man is on a mission to fix the way we sleep</td>\n",
       "      <td>James Proud is a man on a mission to fix ...</td>\n",
       "      <td>Hayley Tsukayama</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142470</th>\n",
       "      <td>2016-12-24</td>\n",
       "      <td>8 overnight casseroles and easy scones for you...</td>\n",
       "      <td>Let’s be honest: Chances are a cold bowl of ce...</td>\n",
       "      <td>Becky Krystal</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142471</th>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>They aren’t just for backpackers. ‘Poshtels’ b...</td>\n",
       "      <td>I’ve been in my room at the Hollander in...</td>\n",
       "      <td>Kate Silver</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142472</th>\n",
       "      <td>2016-12-20</td>\n",
       "      <td>My answer to the holiday sugar glut: Pomegrana...</td>\n",
       "      <td>Candy and sugar have once again invaded our w...</td>\n",
       "      <td>Casey Seidenberg</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142487</th>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>Afghanistan’s vice president is known for brut...</td>\n",
       "      <td>KABUL  —   For 30 years, Gen. Abdurrashid Do...</td>\n",
       "      <td>Pamela Constable</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11751 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date                                              title  \\\n",
       "2547   2016-12-21  U.S. Plans to Step Up Military Campaign Agains...   \n",
       "2551   2016-12-15  272 Slaves Were Sold to Save Georgetown. What ...   \n",
       "2561   2016-10-14  Among Travelers and Commuters, the Homeless St...   \n",
       "2575   2016-10-19  Bus Bombing in Jerusalem Wounds 21 - The New Y...   \n",
       "2581   2016-10-20  Syria Cease-Fire Crumbles as Bombings Kill Doz...   \n",
       "...           ...                                                ...   \n",
       "142469 2016-12-23   This man is on a mission to fix the way we sleep   \n",
       "142470 2016-12-24  8 overnight casseroles and easy scones for you...   \n",
       "142471 2016-12-19  They aren’t just for backpackers. ‘Poshtels’ b...   \n",
       "142472 2016-12-20  My answer to the holiday sugar glut: Pomegrana...   \n",
       "142487 2016-12-23  Afghanistan’s vice president is known for brut...   \n",
       "\n",
       "                                                  content  \\\n",
       "2547    ABU DHABI, United Arab Emirates  —   The Obama...   \n",
       "2551    WASHINGTON  —   The human cargo was loaded on ...   \n",
       "2561    Wilson Silva said he knew the homeless situati...   \n",
       "2575    JERUSALEM  —   A bomb exploded on a bus in Jer...   \n",
       "2581    BEIRUT, Lebanon  —   For 38 straight days, the...   \n",
       "...                                                   ...   \n",
       "142469       James Proud is a man on a mission to fix ...   \n",
       "142470  Let’s be honest: Chances are a cold bowl of ce...   \n",
       "142471        I’ve been in my room at the Hollander in...   \n",
       "142472   Candy and sugar have once again invaded our w...   \n",
       "142487    KABUL  —   For 30 years, Gen. Abdurrashid Do...   \n",
       "\n",
       "                                     author label  \n",
       "2547    Michael S. Schmidt and Eric Schmitt  real  \n",
       "2551                       Rachel L. Swarns  real  \n",
       "2561                        Corey Kilgannon  real  \n",
       "2575                        Isabel Kershner  real  \n",
       "2581                           Anne Barnard  real  \n",
       "...                                     ...   ...  \n",
       "142469                     Hayley Tsukayama  real  \n",
       "142470                        Becky Krystal  real  \n",
       "142471                          Kate Silver  real  \n",
       "142472                     Casey Seidenberg  real  \n",
       "142487                     Pamela Constable  real  \n",
       "\n",
       "[11751 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true[\"date\"] = pd.to_datetime(true[\"date\"])\n",
    "\n",
    "#filter on credible news sources and dates\n",
    "true = true[true[\"publication\"].isin([\"Atlantic\", \"CNN\", \"New York Times\", \"NPR\", \"Reuters\", \"Washington Post\"])]\n",
    "true = true[(true['date'] > '2016-09-15') & (true['date'] < '2016-12-25')]\n",
    "\n",
    "true = true[[\"date\", \"title\", \"content\", \"author\"]]\n",
    "true[\"label\"] = \"real\"\n",
    "true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through rows and perform text pre-processing\n",
    "\n",
    "Now that the data is read in and stored in a DataFrame, the text needs to be pre-processed to prepare it for use in a model. The code below completes the following steps:\n",
    "- Append the title of the article to the body of the article (so that both are included in the processed dat) and then word-tokenize the text (i.e. split it on spaces and punctuation).\n",
    "- Convert all of the tokens to lowercase and remove any remaining punctuation and any tokens that are numeric. \n",
    "- Remove stop words from the tokens (using the NLTK English stop word list)\n",
    "- Perform part of speech tagging (because part of speech is needed for lemmatization)\n",
    "- Reduce words to their root form using lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine real and fake news into one DataFrame\n",
    "news_df = pd.concat([true, fake])\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = []\n",
    "labels = []\n",
    "for i, row in news_df.iterrows():\n",
    "    tokens = nltk.word_tokenize(str(row[\"title\"]) + \" \" + str(row[\"content\"])) #split text into tokens\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()] #convert to lowercase, remove punctuation and numbers\n",
    "    filtered_tokens = [word for word in tokens if word not in stopwords.words('english')] #remove stop words\n",
    "    #perform part of speech tagging and lemmatization\n",
    "    lemmatized_words = []\n",
    "    for word, tag in nltk.pos_tag(filtered_tokens):\n",
    "        pos = tag[0].lower()\n",
    "        pos = pos if pos in ['a', 'r', 'n', 'v'] else None\n",
    "        if not pos:\n",
    "            lemma = word\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word, pos)\n",
    "        lemmatized_words.append(lemma)\n",
    "    processed_text.append(lemmatized_words)\n",
    "    labels.append(row[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code saves the pre-processed text and the corresponding labels (real/fake) to a pickle file so that the task does not need to be repeated in the future. I did this simply because the text pre-processing is a bit time consuming and it's simpler to save the text in its processed state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict = {\"processed_text\": processed_text, \"labels\": labels}\n",
    "with open('processed_text.pickle', 'wb') as file:\n",
    "    pickle.dump(text_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert documents to vectors using bag-of-words method\n",
    "\n",
    "This step involves converting the processed text into numerical vectors using the bag-of-words method. \n",
    "\n",
    "The first block of code loads the pre-processed text and the labels from the pickle file, so that the text does not need to be re-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load processed text from pickle file so that pre-processing doesn't need to be redone\n",
    "with (open(\"processed_text.pickle\", \"rb\")) as file:\n",
    "    contents = pickle.load(file)\n",
    "    processed_text = contents[\"processed_text\"]\n",
    "    labels = contents[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two code blocks concatenate all words from the same document together, and then run the scikit-learn CountVectorizer on the data. This creates a vector that has the occurence count of each word in each document. This code also converts the labels of \"real\" and \"fake\" to the numerical values of 0 and 1. 1 corresponds to \"fake\" and 0 corresponds to \"real\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, document in enumerate(processed_text):\n",
    "    processed_text[i] = \" \".join(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(processed_text)\n",
    "y = [1 if l == \"fake\" else 0 for l in labels ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115377\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last code block prints the number of unqiue features in the data, which is the same as the length of each document vector. This is a very large number and a Bayesian network cannot effectively handle this many features. For that reason, the next step involves selecting the best features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best features using chi<sup>2</sup> score\n",
    "\n",
    "The code below instantiates a feature selector that uses the chi<sup>2</sup> score to find the 10 best features in the data. Each of those features (words) is then printed out along with its chi<sup>2</sup> score and p-value. The purpose of this code is just to test the selector and to see what some of the top words are. Later code will involve iteratively increasing the number of features to keep and then fitting a model on the data with only those features.\n",
    "\n",
    "The chi<sup>2</sup> score will be higher for features that cause a more homogenous split in the data, when considering class label. This means that the top features found using the chi<sup>2</sup> score will be the words that most clearly divide the documents into \"real\" and \"fake\" news categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "      <th>pval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89148</th>\n",
       "      <td>say</td>\n",
       "      <td>41219.589510</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104288</th>\n",
       "      <td>trump</td>\n",
       "      <td>14388.194406</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113408</th>\n",
       "      <td>year</td>\n",
       "      <td>5097.927263</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44880</th>\n",
       "      <td>hillary</td>\n",
       "      <td>4853.445880</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76225</th>\n",
       "      <td>percent</td>\n",
       "      <td>4361.260516</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110708</th>\n",
       "      <td>wednesday</td>\n",
       "      <td>3715.079188</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19959</th>\n",
       "      <td>company</td>\n",
       "      <td>3536.620276</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18408</th>\n",
       "      <td>city</td>\n",
       "      <td>3294.169340</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113733</th>\n",
       "      <td>york</td>\n",
       "      <td>3261.625956</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69577</th>\n",
       "      <td>new</td>\n",
       "      <td>3254.233992</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word         score  pval\n",
       "89148         say  41219.589510   0.0\n",
       "104288      trump  14388.194406   0.0\n",
       "113408       year   5097.927263   0.0\n",
       "44880     hillary   4853.445880   0.0\n",
       "76225     percent   4361.260516   0.0\n",
       "110708  wednesday   3715.079188   0.0\n",
       "19959     company   3536.620276   0.0\n",
       "18408        city   3294.169340   0.0\n",
       "113733       york   3261.625956   0.0\n",
       "69577         new   3254.233992   0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# Select features with highest chi-squared statistics\n",
    "chi2_selector = SelectKBest(chi2, k=10)\n",
    "chi2_selector.fit(X, y)\n",
    "\n",
    "# Look at scores returned from the selector for each feature\n",
    "chi2_scores = pd.DataFrame(list(zip(vectorizer.get_feature_names(), chi2_selector.scores_, chi2_selector.pvalues_)), columns=['word', 'score', 'pval'])\n",
    "\n",
    "chi2_scores.nlargest(10, \"score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These top 10 words are interesting because they are words that we would expect to see in news articles, but are not necessarily expected to clearly distinguish between real and fake news. The top two words, at least, appear to be a result of this particular dataset. Looking back at the portion of the DataFrame that was output after the data was read in, we can see that the last few entries repeat the phrase \"21st Century Wire says\". If there are a lot of articles from this one source, and they frequently start their articles with that phrase, it seems like that is why the word \"say\" is considered to be the best word for distinguishing between real and fake news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and evaluate Bayesian network on different numbers of features\n",
    "\n",
    "Now that we've established a method to select the top k features, we'll run several experiments where we fit a Bayesian network on varying numbers of features and evaluate the resulting model by making predictions. This fits models on 10, 50, 100, 250, 500, 750, and 1000 features. For each of those sets of features, the following steps are completed:\n",
    "\n",
    "- Initialize a k-best feature selector for the current number of features, and transform the data using that selector\n",
    "- Append the class labels to the end of the feature vector for each document, since the Bayes net requires the label to be one of the nodes in the network in order to use inference to predict it\n",
    "- Split the data into 80% training data and 20% test data\n",
    "- Fit the Bayesian network on the training data, using the pomegranate library\n",
    "- Remove the labels from the feature vectors of the test data and then use the model to predict the label of those documents\n",
    "- Compare the predicted value to the true labels (for the test set) and tally up the number of true positives (i.e. label is 1 and prediction is 1), true negatives (i.e. label is 0 and prediction is 0), false positives (i.e. label is 0 and prediction is 1), and false negatives (i.e. label is 1 and prediction is 0).\n",
    "- Use those tallies to compute accuracy, precision, recall, and the F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian network with 10 features\n",
      "Accuracy:  0.7432403661911858\n",
      "Precision:  0.7180871577323563\n",
      "Recall:  0.7967479674796748\n",
      "F1 Score:  0.7553752535496957\n",
      "\n",
      "Bayesian network with 50 features\n",
      "Accuracy:  0.7466467958271237\n",
      "Precision:  0.7349446947972142\n",
      "Recall:  0.7676508344030809\n",
      "F1 Score:  0.7509418166596903\n",
      "\n",
      "Bayesian network with 100 features\n",
      "Accuracy:  0.7577176921439217\n",
      "Precision:  0.7474205530334296\n",
      "Recall:  0.7749251176722294\n",
      "F1 Score:  0.7609243697478991\n",
      "\n",
      "Bayesian network with 250 features\n",
      "Accuracy:  0.7594209069618906\n",
      "Precision:  0.7492771581990912\n",
      "Recall:  0.7762088147197261\n",
      "F1 Score:  0.762505254308533\n",
      "\n",
      "Bayesian network with 500 features\n",
      "Accuracy:  0.7651692569725357\n",
      "Precision:  0.7514262428687857\n",
      "Recall:  0.789045785194694\n",
      "F1 Score:  0.7697766645794197\n",
      "\n",
      "Bayesian network with 750 features\n",
      "Accuracy:  0.7645305514157973\n",
      "Precision:  0.748887990295188\n",
      "Recall:  0.7924689773213521\n",
      "F1 Score:  0.7700623700623701\n",
      "\n",
      "Bayesian network with 1000 features\n",
      "Accuracy:  0.7632531403023206\n",
      "Precision:  0.7474747474747475\n",
      "Recall:  0.7916131792896877\n",
      "F1 Score:  0.768911055694098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_features_list = [10, 50, 100, 250, 500, 750, 1000]\n",
    "for num_features in num_features_list:\n",
    "    chi2_selector = SelectKBest(chi2, k=num_features) # initialize feature selector\n",
    "    X_new = chi2_selector.fit_transform(X, y).toarray() # fit selector and transform the data\n",
    "    \n",
    "    # combine features in labels into one matrix for Bayes net training\n",
    "    y_new = [[label] for label in y]\n",
    "    X_and_y = np.append(X_new, y_new, axis=1)\n",
    "    \n",
    "    # perform 80/20 split for training and test data\n",
    "    training_data, test_data = train_test_split(X_and_y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # fit the model\n",
    "    model = BayesianNetwork.from_samples(training_data, algorithm=\"chow-liu\", n_jobs=3)\n",
    "    \n",
    "    # remove labels from test data and make predictions\n",
    "    test_data_no_labels = []\n",
    "    for entry in test_data:\n",
    "        test_data_no_labels.append(list(entry))\n",
    "        test_data_no_labels[-1][-1] = None\n",
    "    test_data_no_labels = np.array(test_data_no_labels)\n",
    "    predictions = model.predict(test_data_no_labels, check_input=False)\n",
    "    \n",
    "    # evaluate predictions by computing accuracy, precision, recall, and f1 score\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    for prediction, true_value in zip(predictions, test_data):\n",
    "        if prediction[-1] == 1 and true_value[-1] == 1:\n",
    "            true_positives += 1\n",
    "        elif prediction[-1] == 0 and true_value[-1] == 0:\n",
    "            true_negatives += 1\n",
    "        elif prediction[-1] == 1 and true_value[-1] == 0:\n",
    "            false_positives += 1\n",
    "        else:\n",
    "            false_negatives += 1\n",
    "\n",
    "    accuracy = (true_positives + true_negatives) / len(test_data)\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "    print(\"Bayesian network with \" + str(num_features) + \" features\")\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1 Score: \", f1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and evaluate naive Bayes classifier on the same numbers of features\n",
    "\n",
    "Finally, we run the same experiments using a naives Bayes classifier to compare it to the performance of the Bayesian network. I specifically used the Multinomial naive Bayes algorithm because it is known to do well on text classification tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with 10 features\n",
      "Accuracy:  0.6940600383223334\n",
      "Precision:  0.7432432432432432\n",
      "Recall:  0.5883611467693625\n",
      "F1 Score:  0.6567948411750656\n",
      "\n",
      "Naive Bayes with 50 features\n",
      "Accuracy:  0.76218863104109\n",
      "Precision:  0.8028798411122146\n",
      "Recall:  0.6919127086007703\n",
      "F1 Score:  0.7432774074925305\n",
      "\n",
      "Naive Bayes with 100 features\n",
      "Accuracy:  0.7879497551628699\n",
      "Precision:  0.8454404945904173\n",
      "Recall:  0.7021822849807445\n",
      "F1 Score:  0.767180925666199\n",
      "\n",
      "Naive Bayes with 250 features\n",
      "Accuracy:  0.7960400255482223\n",
      "Precision:  0.8604286461055933\n",
      "Recall:  0.7043217800599059\n",
      "F1 Score:  0.7745882352941177\n",
      "\n",
      "Naive Bayes with 500 features\n",
      "Accuracy:  0.8162657015116032\n",
      "Precision:  0.8699799196787149\n",
      "Recall:  0.7415489944373128\n",
      "F1 Score:  0.8006468006468008\n",
      "\n",
      "Naive Bayes with 750 features\n",
      "Accuracy:  0.8290398126463701\n",
      "Precision:  0.8763493621197253\n",
      "Recall:  0.7642276422764228\n",
      "F1 Score:  0.8164571428571429\n",
      "\n",
      "Naive Bayes with 1000 features\n",
      "Accuracy:  0.8358526719182456\n",
      "Precision:  0.8782608695652174\n",
      "Recall:  0.7779204107830552\n",
      "F1 Score:  0.8250510551395506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_features_list = [10, 50, 100, 250, 500, 750, 1000]\n",
    "for num_features in num_features_list:\n",
    "    chi2_selector = SelectKBest(chi2, k=num_features) # initialize feature selector\n",
    "    X_new = chi2_selector.fit_transform(X, y).toarray() # fit selector and transform the data\n",
    "    \n",
    "    # perform 80/20 split for training and test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # fit the model\n",
    "    nb_model = MultinomialNB().fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions on test set\n",
    "    predictions = nb_model.predict(X_test)\n",
    "    \n",
    "    # evaluate predictions by computing accuracy, precision, recall, and f1 score\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    for prediction, true_value in zip(predictions, y_test):\n",
    "        if prediction == 1 and true_value == 1:\n",
    "            true_positives += 1\n",
    "        elif prediction == 0 and true_value == 0:\n",
    "            true_negatives += 1\n",
    "        elif prediction == 1 and true_value == 0:\n",
    "            false_positives += 1\n",
    "        else:\n",
    "            false_negatives += 1\n",
    "\n",
    "    accuracy = (true_positives + true_negatives) / len(y_test)\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "    print(\"Naive Bayes with \" + str(num_features) + \" features\")\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1 Score: \", f1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Bayesian network with 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 10\n",
    "chi2_selector = SelectKBest(chi2, k=num_features) # initialize feature selector\n",
    "X_new = chi2_selector.fit_transform(X, y).toarray() # fit selector and transform the data\n",
    "\n",
    "chi2_scores = pd.DataFrame(list(zip(vectorizer.get_feature_names(), chi2_selector.scores_, chi2_selector.pvalues_)), columns=['word', 'score', 'pval'])\n",
    "\n",
    "feature_names = list(chi2_scores.nlargest(num_features, \"score\")[\"word\"])\n",
    "feature_names.append(\"*label*\")\n",
    "    \n",
    "# combine features in labels into one matrix for Bayes net training\n",
    "y_new = [[label] for label in y]\n",
    "X_and_y = np.append(X_new, y_new, axis=1)\n",
    "\n",
    "# perform 80/20 split for training and test data\n",
    "training_data, test_data = train_test_split(X_and_y, test_size=0.2, random_state=42)\n",
    "    \n",
    "# fit the model\n",
    "model = BayesianNetwork.from_samples(training_data, algorithm=\"chow-liu\", root=0, n_jobs=3, state_names=feature_names)\n",
    "model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "model.plot(filename=\"Bayes_net_10_features.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
